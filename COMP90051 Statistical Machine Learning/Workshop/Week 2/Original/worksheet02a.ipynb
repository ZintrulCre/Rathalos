{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 2\n",
    "## Part A: Linear regression\n",
    "\n",
    "***\n",
    "\n",
    "Our aim for this part of the workshop is to fit a linear model from scratch—relying only on the `numpy` library. We'll experiment with two implementations: one based on iterative updates (coordinate descent) and another based on linear algebra. Finally, to check the correctness of our implementation, we'll compare its output to the output of `sklearn`.\n",
    "\n",
    "Firstly we will import the relevant libraries (`numpy`, `matplotlib`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check what a command does simply type `object?`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Review\n",
    "In lectures, we saw that a linear model can be expressed as:\n",
    "$$y = w_0 + \\sum_{j = 1}^{m} w_j x_j = \\mathbf{w} \\cdot \\mathbf{x} $$\n",
    "where \n",
    "\n",
    "* $y$ is the *target variable*;\n",
    "* $\\mathbf{x} = [x_1, \\ldots, x_m]$ is a vector of *features* (we define $x_0 = 1$); and\n",
    "* $\\mathbf{w} = [w_0, \\ldots, w_m]$ are the *weights*.\n",
    "\n",
    "To fit the model, we *minimise* the empirical risk with respect to $\\vec{w}$. In this simplest case (square loss), this amounts to minimising the sum of squared residuals:\n",
    "\n",
    "$$SSR(\\mathbf{w}) = \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\mathbf{x}_i)^2$$\n",
    "\n",
    "**Note:** For simplicity, we'll consider the case $m = 1$ (i.e. only one feature excluding the intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data set\n",
    "We'll be working with some data from the Olympics—the gold medal race times for marathon winners from 1896 to 2012. The code block below reads the data into a numpy array of floats, and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file with variables YEAR,TIME\n",
    "csv = \"\"\"1896,4.47083333333333\n",
    "1900,4.46472925981123\n",
    "1904,5.22208333333333\n",
    "1908,4.1546786744085\n",
    "1912,3.90331674958541\n",
    "1920,3.5695126705653\n",
    "1924,3.8245447722874\n",
    "1928,3.62483706600308\n",
    "1932,3.59284275388079\n",
    "1936,3.53880791562981\n",
    "1948,3.6701030927835\n",
    "1952,3.39029110874116\n",
    "1956,3.43642611683849\n",
    "1960,3.2058300746534\n",
    "1964,3.13275664573212\n",
    "1968,3.32819844373346\n",
    "1972,3.13583757949204\n",
    "1976,3.07895880238575\n",
    "1980,3.10581822490816\n",
    "1984,3.06552909112454\n",
    "1988,3.09357348817\n",
    "1992,3.16111703598373\n",
    "1996,3.14255243512264\n",
    "2000,3.08527866650867\n",
    "2004,3.1026582928467\n",
    "2008,2.99877552632618\n",
    "2012,3.03392977050993\"\"\"\n",
    "\n",
    "# Read into a numpy array (as floats)\n",
    "olympics = np.genfromtxt(io.BytesIO(csv.encode()), delimiter=\",\")\n",
    "print(olympics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the race time as the *target variable* $y$ and the year of the race as the only non-trivial *feature* $x = x_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = olympics[:, 0:1]\n",
    "y = olympics[:, 1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting $y$ vs $x$, we see that a linear model could be a decent fit for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'rx')\n",
    "plt.ylabel(\"y (Race time)\")\n",
    "plt.xlabel(\"x (Year of race)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterative solution (coordinate descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding out the sum of square residuals for this simple case (where $\\mathbf{w}=[w_0, w_1]$) we have:\n",
    "$$SSR(w_0, w_1) = \\sum_{i=1}^{n}(y_i - w_0 - w_1 x_i)^2$$\n",
    "Let's start with an initial guess for the slope $w_1$ (which is clearly negative from the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = -0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the maximum likelihood update, we get the following estimate for the intercept $w_0$:\n",
    "$$w_0 = \\frac{\\sum_{i=1}^{n}(y_i-w_1 x_i)}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w0(x, y, w1):\n",
    "    return ... # fill in\n",
    "\n",
    "w0 = update_w0(x, y, w1)\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can update $w_1$ based on this new estimate of $w_0$:\n",
    "$$w_1 = \\frac{\\sum_{i=1}^{n} (y_i - w_0) \\times x_i}{\\sum_{i=1}^{n} x_i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w1(x, y, w0):\n",
    "    return ... # fill in\n",
    "\n",
    "w1 = update_w1(x, y, w0)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the quality of fit for these values for the weights $w_0$ and $w_1$. We create a vector of \"test\" values `x_test` and a function to compute the predictions according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.arange(1890, 2020)[:, None]\n",
    "\n",
    "def predict(x_test, w0, w1): \n",
    "    return ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the test predictions with a blue line on the same plot as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(x_test, y_test, x, y): \n",
    "    plt.plot(x_test, y_test, 'b-')\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylabel(\"y (Race time)\")\n",
    "    plt.xlabel(\"x (Year of race)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_fit(x_test, predict(x_test, w0, w1), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compute the sum of square residuals $SSR(w_0,w_1)$ on the training set to measure the goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SSR(x, y, w0, w1): \n",
    "    return ... # fill in\n",
    "\n",
    "print(compute_SSR(x, y, w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious from the plot that the fit isn't very good. \n",
    "We must repeat the alternating parameter updates many times before the algorithm converges to the optimal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(10000):\n",
    "    w1 = update_w1(x, y, w0) \n",
    "    w0 = update_w0(x, y, w1) \n",
    "    if i % 500 == 0:\n",
    "        print(\"Iteration #{}: SSR = {}\".format(i, compute_SSR(x, y, w0, w1)))\n",
    "print(\"Final estimates: w0 = {}; w1 = {}\".format(w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try plotting the result again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(x_test, predict(x_test, w0, w1), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does more than 10 iterations considerably improve fit in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear algebra solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lectures, we saw that it's possible to solve for the optimal weights $\\mathbf{w}^\\star$ analytically. The solution is\n",
    "$$\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "where\n",
    "$$\\mathbf{X} = \\begin{pmatrix} \n",
    "        1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \n",
    "    \\end{pmatrix} \n",
    "  \\quad \\text{and} \\quad \n",
    "  \\mathbf{y} = \\begin{pmatrix} \n",
    "          y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We construct $\\mathbf{X}$ in the code block below, remembering to include the $x_0 = 1$ column for the bias (intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones_like(x), x))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can express $\\mathbf{w}^\\star$ explicitly in terms of the matrix inverse $(\\mathbf{X}^\\top \\mathbf{X})^{-1}$, this isn't an efficient way to compute $\\mathbf{w}$ numerically. It is better instead to solve the following system of linear equations:\n",
    "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}^\\star = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "This can be done in numpy using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ... # fill in\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this solution, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = w\n",
    "plot_fit(x_test, predict(x_test, w0, w1), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should verify that the sum of squared residuals $SSR(w_0, w_1)$, match or beats the earlier iterative result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_SSR(x, y, w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The error we computed above is the *training* error. It doesn't assess the model's generalization ability, it only assesses how well it's performing on the given training data. In later worksheets we'll assess the generalization ability of models using held-out evaluation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Solving using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a good understanding of what's going on under the hood, you can use the functionality in `sklearn` to solve linear regression problems you encounter in the future. Using the `LinearRegression` module, fitting a linear regression model becomes a one-liner as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` module provides access to the bias weight $w_0$ under the `intercept_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the non-bias weights under the `coef_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should check that these results match the solution you obtained previously. Note that sklearn also uses a numerical linear algebra solver under the hood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
